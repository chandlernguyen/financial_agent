{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from typing import Iterator\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "\n",
    "class CustomHTMLLoader(BaseLoader):\n",
    "    \"\"\"\n",
    "    Custom document loader that parses HTML files to extract text and embedded metadata.\n",
    "    \n",
    "    This loader is specifically designed to handle the cleaned HTM documents, extracting the main text content and any metadata \n",
    "    defined within <meta> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the document loader with a specific HTM file path.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): The filesystem path to the HTM file that will be processed by this loader.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"\n",
    "        Lazily loads content and metadata from an HTM file.\n",
    "        \n",
    "        This method opens the specified HTM file, reads its contents, and extracts both the text content and any\n",
    "        relevant metadata stored in <meta> tags. It yields a Document object containing this data, suitable for \n",
    "        use in various NLP contexts within the Langchain framework.\n",
    "        \n",
    "        Yields:\n",
    "            Document: An object containing the extracted content and metadata.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Extract main textual content from the <body> element, if available; otherwise, use the entire HTML text.\n",
    "        main_content = soup.find('body').text if soup.find('body') else soup.text\n",
    "\n",
    "        # Collect metadata from <meta> tags; specifically, we look for tags with both 'name' and 'content' attributes.\n",
    "        metadata = {meta['name']: meta['content'] for meta in soup.find_all('meta') if 'name' in meta.attrs and 'content' in meta.attrs}\n",
    "\n",
    "        # Optionally include the path to the HTML source file in metadata for reference in debugging or logging.\n",
    "        # Comment out or remove the following line in production to avoid exposing file paths.\n",
    "        # metadata['source'] = self.file_path\n",
    "\n",
    "        yield Document(page_content=main_content, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script processes HTML documents from a designated directory by first loading the documents using the above custom loader,\n",
    "and then dividing the content of each document into smaller, overlapping chunks. \n",
    "\n",
    "The script sets each chunk to 1024 characters with a 128-character overlap between chunks to ensure continuity of context.\n",
    "You can adjust the chunk size and overlap as needed based on the requirements of your LLM application.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Define the directory path for cleaned HTML documents. Modify the path as per your local setup.\n",
    "cleaned_filings_dir = \"path/to/cleaned_filings_directory\"\n",
    "\n",
    "# Initialize the text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "\n",
    "# List to collect all document objects\n",
    "documents = []\n",
    "\n",
    "# Loop through each file in the specified directory that ends with '.htm'\n",
    "for filename in os.listdir(cleaned_filings_dir):\n",
    "    if filename.endswith('.htm'):\n",
    "        # Generate the full path for each HTML file\n",
    "        file_path = os.path.join(cleaned_filings_dir, filename)\n",
    "\n",
    "        # Create an instance of the HTML loader for the current file\n",
    "        loader = CustomHTMLLoader(file_path)\n",
    "\n",
    "        # Load and append the document from the HTML file to the documents list\n",
    "        documents.extend(loader.lazy_load())  # 'extend' is used to merge lists of documents\n",
    "\n",
    "# After collecting all documents, split them into chunks using the defined text splitter\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Optionally print or log the number of documents and chunks processed\n",
    "print(f\"Processed {len(documents)} documents into {len(chunks)} chunks.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
